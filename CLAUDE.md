# AI Pulse - プロジェクトの文脈と目的

## プロジェクトの背景

このプロジェクトは、**毎日ブログを書くためのネタ探しを自動化する実用的なツール**です。

### 開発者の状況
- 毎日ブログを書いている（1年間継続中）
- 毎日ネタ探しに時間を取られている
- 以下の作業を手動で行っている：
  - 各社生成AIのアップデート情報をチェック
  - 新モデルのベンチ結果をリサーチ
  - AnthropicやOpenAIの公式発表を確認
  - ユーザーの声を集める

### プロジェクトの目的
**最新情報や便利情報を集めて、ブログネタのアイデアを自動生成する。そして、ある程度使えるレベルまで高めること。**

## 重要な設計思想

### 1. 実用性重視
- これは「実験的なプロジェクト」ではなく「毎日使う実用ツール」
- 「完璧」より「使える」ことを優先
- 動作することが最優先、過度な最適化は後回し

### 2. ブログネタ生成が目的
- すべての機能は「ブログネタとして使えるか」を基準に判断
- 情報の価値は「記事に書けるかどうか」で評価
- ダッシュボードは「選ぶだけ」で使えることを重視

### 3. データの価値判断基準

#### 高優先度（ブログネタとして価値が高い）
- **公式発表**: OpenAI、Anthropic、Google DeepMind、xAIなどの公式情報
- **新モデルリリース**: 新機能、性能向上、価格変更など
- **ベンチマーク結果**: Arenaスコアの変動、新モデルの性能比較
- **急上昇トレンド**: 複数ソースで同時に言及されている話題
- **感情の大きな変化**: ポジティブ/ネガティブの急激な変化

#### 中優先度
- 技術コミュニティの反応（Reddit、HN、HuggingFace）
- 一般ユーザーの声（Twitter/X）
- 継続的なトレンド（緩やかな上昇）

#### 低優先度
- 古い情報や重複
- 単一ソースのみの言及
- 感情変化のない情報

## システムの動作フロー

```
1. データ収集（自動クローリング）
   ↓
2. raw_events テーブルに保存（生データ）
   ↓
3. データ整理・分析
   - model_updates: モデルごとのアップデート整理
   - user_voices: コミュニティの声を要約
   - trends: トレンド分析結果
   ↓
4. AIによる統合サマリー生成
   - 公式情報（事実）
   - 技術者の声（専門性）
   - 一般ユーザーの感情（本音）
   - トレンド情報（急上昇・注目度）
   ↓
5. ブログ候補の生成
   - 複数の情報源を統合
   - 記事ネタとして使える形に整理
   ↓
6. ダッシュボードで確認・選択
   - 「採用」: 記事にする
   - 「保留」: 後で確認
   - 「ボツ」: 使わない
```

## データモデルの意味と関係性

### raw_events（生データ）
- **役割**: 各クローラーが取得した生のデータを保存
- **用途**: 後で分析・集計するための元データ
- **特徴**: 重複やノイズを含む可能性がある

### model_updates（モデルアップデート）
- **役割**: モデルごとのアップデートを整理
- **用途**: 特定モデルの変更履歴を追跡
- **関係**: raw_events から抽出・整理されたデータ

### user_voices（ユーザーの声）
- **役割**: Reddit/X/HN の口コミを要約
- **用途**: コミュニティの反応や感情を把握
- **関係**: raw_events から感情分析・要約されたデータ

### trends（トレンド分析）
- **役割**: 急上昇キーワード、感情変化、言及数推移を保存
- **用途**: トレンドの可視化とブログネタの優先順位付け
- **関係**: raw_events を時系列で分析した結果

### blog_ideas（ブログ候補）
- **役割**: 記事ネタ候補を管理
- **用途**: ダッシュボードで「採用/保留/ボツ」を選択
- **関係**: summarize-today で生成された統合サマリーから作成

### logs（バッチログ）
- **役割**: 各APIの実行ログを記録
- **用途**: エラー追跡、実行状況の確認

## 各APIエンドポイントの詳細な役割

### `/api/fetch-official`
- **目的**: 公式情報を取得（RSS/API）
- **対象**: OpenAI、Anthropic、Google DeepMind、xAI、Microsoft AI Blog
- **実行頻度**: 毎時
- **重要度**: 高（公式発表は最優先）

### `/api/fetch-community`
- **目的**: 技術コミュニティの反応を取得
- **対象**: HackerNews、Reddit、HuggingFace、GitHub Issues
- **実行頻度**: 3時間ごと
- **重要度**: 中（専門家の意見）

### `/api/fetch-twitter`
- **目的**: 一般ユーザーの声を取得
- **対象**: X（Twitter）投稿
- **実行頻度**: 適宜
- **重要度**: 中（感情データ）

### `/api/fetch-arena`
- **目的**: 客観的な性能データを取得
- **対象**: LMSYS/Arenaスコア
- **実行頻度**: 毎時
- **重要度**: 高（ベンチマーク結果）

### `/api/analyze-trends`
- **目的**: トレンド分析を実行
- **処理内容**:
  - 過去N日間との比較（言及数の増加率）
  - 急上昇キーワードの検出
  - 感情の変化トレンド
  - Arenaスコアの変動トレンド
  - 複数ソースでの同時言及の検出
- **実行頻度**: 6時間ごと
- **重要度**: 高（ブログネタの優先順位付け）

### `/api/summarize-today`
- **目的**: 収集データをLLMで統合サマリー化
- **処理内容**:
  - 公式情報（事実）の整理
  - 技術者の声（専門性）の要約
  - 一般ユーザーの感情（本音）の分析
  - Arenaなどの客観スコアの統合
  - トレンド情報（急上昇・注目度）の反映
  - 記事ネタ候補の生成
- **実行頻度**: 毎朝8時
- **重要度**: 最高（ブログネタ生成の核心）

### `/api/push-blog-idea`
- **目的**: ブログ候補をDBに登録
- **処理内容**: summarize-today で生成された候補を blog_ideas テーブルに保存
- **重要度**: 高（ダッシュボードで選択可能にする）

## ブログネタの選定基準

### 「採用」にする基準
- ✅ 公式発表がある（事実ベース）
- ✅ トレンドが急上昇している（注目度が高い）
- ✅ 複数ソースで同時に言及されている（確度が高い）
- ✅ 感情の大きな変化がある（話題性がある）
- ✅ Arenaスコアの変動が大きい（性能面での変化）

### 「保留」にする基準
- ⏸️ 情報が不足している（追加情報待ち）
- ⏸️ まだトレンドが明確でない（様子見）
- ⏸️ 複数の情報源を待っている

### 「ボツ」にする基準
- ❌ 古い情報や重複
- ❌ 単一ソースのみの言及（確度が低い）
- ❌ 感情変化のない情報（話題性がない）
- ❌ ブログネタとして使えない内容

## 開発時の優先順位

### 最優先（すぐに使えるように）
1. 基本的なデータ収集機能（fetch-official, fetch-community）
2. ダッシュボードの基本表示
3. ブログ候補の生成と表示

### 次に優先（実用性向上）
1. トレンド分析機能
2. 感情分析機能
3. 統合サマリー生成

### 後回し（最適化）
1. パフォーマンス最適化
2. UI/UXの細かい改善
3. エッジケースの対応

## 実装時の注意点

### エラーハンドリング
- 外部APIのエラーはログに記録して続行
- 一部のデータ取得に失敗しても、他のデータは取得する
- ユーザーには分かりやすいエラーメッセージを表示

### パフォーマンス
- 大量データの取得時はページネーションを実装
- トレンド分析は定期的に実行し、結果をキャッシュ（trendsテーブル）
- リアルタイム計算は避ける

### データの品質
- 重複データの排除
- 古いデータの自動削除（必要に応じて）
- データの信頼性を考慮（公式情報を優先）

## 目標

**「毎日、ブログネタが降ってくる状態をつくる」**

そのために：
- 自動化を徹底（手動作業を最小限に）
- 実用性を重視（完璧を求めすぎない）
- 継続的に改善（使ってみて問題があれば修正）

